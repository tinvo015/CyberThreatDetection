{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cyber Threat Detection Using Machine Learning\n",
    "\n",
    "You will import the dataset that consists of multiple cyber threat traces and apply Machine Learning techniques to detect cyber threats: \n",
    "\n",
    "* Unsupervised Learning: **Kernel Principal Componenet Analysis (PCA)**\n",
    "* Supervised Learning: 1) **k-Nearest Neighbor (kNN)** and 2) **Deep Neural Network (DNN)**\n",
    "\n",
    "The organization of this project is as follows:\n",
    "- Section 1: Packages\n",
    "- Section 2: Data Preparation\n",
    "- Section 3: Unsupervised Learning - Kernel PCA\n",
    "- Section 4: Supervised Learning I - kNN\n",
    "- Section 5: Supervised Learning II - DNN\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "Let's first import the packages that you will need for this project. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a library to plot graphs in Python.\n",
    "- [csv](https://docs.python.org/3/library/csv.html) is the most common import and export format for spreadsheets and databases.\n",
    "- [pickle](https://docs.python.org/3/library/pickle.html) implements binary protocols for serializing and de-serializing a Python object structure.\n",
    "\n",
    "Some useful iPython Notebook keyboard shortcuts:\n",
    "- `Shift-Enter`: run cell (and move to the next cell)\n",
    "- `Ctrl-Enter`: run cell in-place\n",
    "- `Alt-Enter`: run cell and insert a new cell below\n",
    "\n",
    "More tricks are summarized here:\n",
    "- http://johnlaudun.org/20131228-ipython-notebook-keyboard-shortcuts/\n",
    "- https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Data Preparation\n",
    "\n",
    "We have already preprocessed the log file and store the processed data in a pickle file. You just need to import the dataset from the pickle file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider five different cyber attacks: \n",
    "1. NFS attack\n",
    "2. Trojan attack\n",
    "3. Apache attack\n",
    "4. UDP DDoS attack\n",
    "5. HTTP DDoS attack\n",
    "\n",
    "Hence, we can define a `dictionary` called `data_labels` that includes six different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = {'normal': 0, \n",
    "               'nfs': 1,\n",
    "               'trojan': 2,\n",
    "               'apache': 3,\n",
    "               'ddos-UDP': 4,\n",
    "               'ddos-http': 5\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are a total of {} different classes:'.format(len(data_labels)))\n",
    "\n",
    "for label in data_labels:\n",
    "    print(\"  Class {}: {}\".format(data_labels[label], label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, you want to know the class ID for 'apache' attack. Type the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels['apache']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Import dataset\n",
    "\n",
    "Next, we are going to open the provided pickle file and load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./DATA/converted_data.pickle', 'rb') as file:\n",
    "    imported_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object `imported_data` is a dictionary that stores datasets with different labels in the `(key, value)` format, where the `key` is the label and the `value` is the corresponding dataset. So let's check the keys in `imported_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(imported_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we focus on nfs, trojan, apache, DDoS-UDP, and DDoS-HTTp attacks, and ignore the dynamic-hulk and static-hulk attacks (they are variants of DDoS attacks).\n",
    "* To retrieve the dataset with labal `normal`, use `imported_data['normal']`.\n",
    "* The dataset for each label is 3D list.\n",
    "* To access the `(i,j,k)`-th entry, type `imported_data['normal'][i][j][k]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value in the 10-th row and 1st col\n",
    "print(imported_data['normal'][10][1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us print the dimension of all datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in data_labels:\n",
    "    num_examples = len(imported_data[label])    # number of rows\n",
    "    num_features = len(imported_data[label][0]) # number of columns\n",
    "    print(\"imported_data['{}']: ({}, {}, {})\".format(label, num_examples, num_features, len(imported_data['normal'][0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Convert 3D list to 2D numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first convert the list to numpy array and print its dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in data_labels:\n",
    "    imported_data[label] = np.array(imported_data[label], dtype=float) # Convert the list to a numpy array of type float\n",
    "    print(imported_data[label].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, each dataset is a numpy array. You may access the `(i,j,k)`-th entry as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"For the 10-th example of the 'normal' data, its 20-th feature is\")\n",
    "print(imported_data['normal'][10, 20, 0])\n",
    "\n",
    "print(\"\\nFor the 20-th example of the 'apache' data, print 0-st through 9-th features.\")\n",
    "print(imported_data['apache'][20, 0:10, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While each dataset is a 3-D array, the third dimension is only 1, which means that we can convert this 3-D array to a 2-D array without lossing any information.\n",
    "\n",
    "**Task 1: Reshape the 3-D array to a 2-D array.**\n",
    "\n",
    "Hints:\n",
    "1. Use the built-in `reshape` method for numpy arrays ([reference](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.reshape.html)).\n",
    "2. Alternatively, you may use the `np.squeeze` to remove single-dimensional entries from the shape of an array ([reference](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.squeeze.html)).\n",
    "\n",
    "Expected output:\n",
    "- `normal: (500, 33292)`\n",
    "- `nfs: (200, 33292)`\n",
    "- `trojan: (200, 33292)`\n",
    "- `apache: (200, 33292)`\n",
    "- `ddos-UDP: (100, 33292)`\n",
    "- `ddos-http: (100, 33292)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "for label in data_labels:\n",
    "    \n",
    "    #################################################################\n",
    "    ###### Start code here ######\n",
    "\n",
    "    ###### End code here ######\n",
    "    #################################################################\n",
    "    \n",
    "    print('{}: {}'.format(label, data[label].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code checks if you have correctly implemented Task 1.\n",
    "# You will not see AssertionError if you implement it corretly.\n",
    "for label in data_labels:\n",
    "    assert(imported_data[label].shape[0] == data[label].shape[0])\n",
    "    assert(imported_data[label].shape[1] == data[label].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Divide the dataset into training, validation, and testing datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, we will usually divide a dataset into training, validation, and testing subsets.\n",
    "* **Training set** is used to train a ML model or classifier.\n",
    "* **Validation set** is used to validate and improve the trained model.\n",
    "* **Testing set** is used to evaluate the performance of the trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **60%** of the samples for training, **20%** for validation, and the remaining **20%** for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the ratios for training, validation, and testing sets\n",
    "training_ratio = 0.6\n",
    "validation_ratio = 0.2\n",
    "testing_ratio = 0.2\n",
    "\n",
    "# We store the size of training set for different datasets\n",
    "train_size = dict()\n",
    "for label in data_labels:\n",
    "    train_size[label] = int(data[label].shape[0] * training_ratio)\n",
    "    \n",
    "# We store the size of testing set for different datasets\n",
    "test_size = dict()\n",
    "for label in data_labels:\n",
    "    test_size[label] = int(data[label].shape[0] * testing_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Prepare the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array([], dtype=float).reshape(0, 33292)  # num_features = 33292\n",
    "train_y = np.array([], dtype=float).reshape(0, 1)\n",
    "\n",
    "for label in data_labels:\n",
    "    class_ID = data_labels[label]  # Get the class ID for the given label\n",
    "    \n",
    "    total_num = data[label].shape[0]  # Count the total number of examples\n",
    "    training_num = int(total_num * training_ratio) # Compute the number of training examples\n",
    "    \n",
    "    start_idx = 0 # Start index of the training set\n",
    "    end_idx = start_idx + training_num # End index of the training set\n",
    "    \n",
    "    training_data = data[label][start_idx:end_idx, :] # Obtain the training data\n",
    "    training_labels = np.ones((training_num, 1)) * class_ID # Create labels for the training data\n",
    "    \n",
    "    # Concatenate the training samples and labels for each dataset\n",
    "    train_x = np.concatenate((train_x, training_data))\n",
    "    train_y = np.concatenate((train_y, training_labels))\n",
    "    \n",
    "    print('Training set: {} examples for Class {} ({})'.format(training_num, class_ID, label))\n",
    "    \n",
    "print('Total number of training examples: {}'.format(train_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the training set\n",
    "assert(train_x.shape == (780, 33292))\n",
    "assert(train_y.shape == (780, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Prepare the validation set.\n",
    "\n",
    "**TASK 2.1: Take the next 20% samples of each dataset and concatenate them to construct the validation set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vali_x = np.array([], dtype=float).reshape(0, 33292)  # num_features = 33292\n",
    "vali_y = np.array([], dtype=float).reshape(0, 1)\n",
    "\n",
    "for label in data_labels:\n",
    "    class_ID = data_labels[label]\n",
    "    \n",
    "    #################################################################\n",
    "    ##### Start code here #####\n",
    "    \n",
    "    ##### End code here #####\n",
    "    #################################################################\n",
    "    \n",
    "    print('Validation set: {} examples for Class {} ({})'.format(validation_num, class_ID, label))\n",
    "\n",
    "print('Total number of validation examples: {}'.format(vali_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the training set\n",
    "assert(vali_x.shape == (260, 33292))\n",
    "assert(vali_y.shape == (260, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Prepare the testing set.\n",
    "\n",
    "**TASK 2.2: Take the last 20% samples of each dataset and concatenate them to construct the testing set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.array([], dtype=float).reshape(0, 33292)  # num_features = 33292\n",
    "test_y = np.array([], dtype=float).reshape(0, 1)\n",
    "\n",
    "for label in data_labels:\n",
    "    class_ID = data_labels[label]\n",
    "    \n",
    "    #################################################################\n",
    "    ###### Start code here ######\n",
    "    \n",
    "    ###### End code here ######\n",
    "    #################################################################\n",
    "    \n",
    "    print('Testing set: {} examples for Class {} ({})'.format(test_num, class_ID, label))\n",
    "\n",
    "print('Total number of testing examples: {}'.format(test_x.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the size of the testing set\n",
    "assert(test_x.shape == (260, 33292))\n",
    "assert(test_y.shape == (260, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Unsupervised Learning - Kernal PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine Learning, unsupervised learning requires an algorithm to infer a function that describesthe structure of unlabeled data, which means data that has not been categorized. We will try tobuild a model to distinguish benign graphs from abnormal graphs that is built from a set of rawunlabeled graphs by grouping graphs based on certain similarity automatically. To achieve this, a common way is to try to extract the key features of the data, which means that we need to lowerthe dimension of the original graph data. \n",
    "\n",
    "[KernelPCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html): Non-linear dimensionality reduction through the use of kernels. A list of frequently used arguments are summarized as follows. \n",
    "- `n_components` : *int, default=None*. Number of components. If None, all non-zero components are kept.\n",
    "- `kernel`: *“linear” | “poly” | “rbf” (Radial Basis Function) | “sigmoid” | “cosine” | “precomputed”*. Kernel. Default=”linear”.\n",
    "- `gamma`: *float, default=1/n_features*. Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by other kernels.\n",
    "- `fit_inverse_transform`: *bool, default=False*. Learn the inverse transform for non-precomputed kernels. (i.e. learn to find the pre-image of a point)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [sklearn](https://scikit-learn.org/stable/): scikit-learn is a widely used Python package for Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Perform Kernel PCA with 3 Principal Components\n",
    "\n",
    "**TASK 3: Apply Kernel PCA on the training dataset with three principal components (`n_components=3`), radial basis function kernel (`kernel=\"rbf\"`), `fix_inverse_transform=True`, and `gamma=10`.**\n",
    "- Instead of applying the KernelPCA transformer to `train_x`, apply it on `train_x_copy`. \n",
    "- Name the projected low-dimensional training set as `train_x_pca`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_copy = copy.deepcopy(train_x)\n",
    "\n",
    "#################################################################\n",
    "##### Start code here. #####\n",
    "# Step 1: Prepare the transformer\n",
    "\n",
    "# Step 2: Fit the model to train_x_copy and transform train_x_copy.\n",
    "\n",
    "##### End code here. #####\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(train_x_pca.shape[1] == 3)\n",
    "\n",
    "print(\"Before PCA, 'train_x' has {} dimensions.\".format(train_x.shape[1]))\n",
    "print(\"After PCA, 'train_x_pca' has {} dimensions.\".format(train_x_pca.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - 3D Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to visualize the transformed data in a 3D plot and color datasets with different labels using different colors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a figure \n",
    "fig = plt.figure(figsize=(13,8))\n",
    "ax = plt.axes(projection='3d')\n",
    "plt.title(\"Kernel PCA Visualization\", fontsize=16)\n",
    "\n",
    "# List of colors\n",
    "colors = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'brown']\n",
    "\n",
    "# Plotting data points in the 3D plot with labels.\n",
    "idx = 0\n",
    "label_idx = 0\n",
    "\n",
    "for label in data_labels:\n",
    "    ax.scatter3D(xs=train_x_pca[idx : idx + train_size[label], 0], \\\n",
    "                ys=train_x_pca[idx : idx + train_size[label], 1], \\\n",
    "                zs=train_x_pca[idx : idx + train_size[label], 2], \\\n",
    "                c = colors[label_idx], \\\n",
    "                label = label)\n",
    "    idx = idx + train_size[label]\n",
    "    label_idx = label_idx + 1\n",
    "\n",
    "# Generate labels.\n",
    "ax.set_xlabel('X-Axis')\n",
    "ax.set_ylabel('Y-Axis')\n",
    "ax.set_zlabel('Z-Axis')\n",
    "\n",
    "# Generate the legend.\n",
    "plt.legend(prop={'size': 16})\n",
    "\n",
    "# Adjust the 3D view.\n",
    "ax.azim = 10\n",
    "ax.elev = 30\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - 2D Visualization\n",
    "\n",
    "**Task 4: In this task, you will visualize the data in 2D. More specifically, you will**\n",
    "* First apply kernel PCA with `n_components=2`, `kernel=\"rbf\"`, `fit_inverse_transform=True`, and `gamma=10`. \n",
    "* Then visualize the data on a 2D figure using the `plt.scatter()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_pca_2d = None\n",
    "\n",
    "#################################################################\n",
    "##### Start code here #####\n",
    "\n",
    "##### End code here #####\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a figure \n",
    "fig = plt.figure(figsize=(13,8))\n",
    "plt.title(\"Kernel PCA 2D Visualization\", fontsize=16)\n",
    "\n",
    "# List of colors\n",
    "colors = ['r', 'b', 'g', 'y', 'c', 'm', 'k', 'brown']\n",
    "\n",
    "# Plotting data points in the 2D plot with labels.\n",
    "#################################################################\n",
    "##### Start code here #####\n",
    "\n",
    "##### End code here #####\n",
    "#################################################################\n",
    "    \n",
    "# Generate labels.\n",
    "plt.xlabel('X-Axis')\n",
    "plt.ylabel('Y-Axis')\n",
    "\n",
    "# Generate the legend.\n",
    "plt.legend(prop={'size': 16})\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5: Based on the 2D and 3D plots, answer the following questions.**\n",
    "- Comparison between the attack data against the normal data:\n",
    "    - What attacks are significantly different from the normal data?\n",
    "    - Which attack is the closest to the normal data?\n",
    "- Comparison among different attacks:\n",
    "    - Which attacks are very similar to each other?\n",
    "    - Which attacks are very different from others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Supervised Learning - *k*-Nearest Neighbor\n",
    "\n",
    "In this section, we will apply a popular unsupervised ML technique called ***k*-Nearest Neighbor**(*k*-means clustering) ([Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm)) to the projected 2D data points and visualize the clustering result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Import Packages and Train *k*-NN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will import the needed packages and run `KNeighborsClassifier` provided by `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "n_neighbors = 15  # It is the `k` value\n",
    "weights = 'uniform'\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "knn_clf.fit(train_x_pca_2d, train_y.ravel()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Evaluate the *k*-NN Classifier on Testing Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 6: Evaluate the kNN classifer on the testing set and print the classification accuracy.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use the trained classifier to make predictions on the testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to also project the testing data onto the same 2D plane. \n",
    "test_x_pca_2d = transformer.transform(copy.deepcopy(test_x))\n",
    "\n",
    "#################################################################\n",
    "##### Start code here #####\n",
    "\n",
    "##### End code here #####\n",
    "#################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Visualization of *k*-NN Decision Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to plot kNN decision boundaries, training data points, and testing data points on the same figure. \n",
    "\n",
    "**Task 7: Plot the decision bounaries of the above kNN classifier.**\n",
    "- Set the resolution to `h=0.05`\n",
    "- Use the color map `cmap=cmap_custom` and set transparency `alpha=0.05`\n",
    "- Useful reference: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,8))\n",
    "\n",
    "cmap_custom = ListedColormap(['r', 'b', 'g', 'y', 'c', 'm'])\n",
    "\n",
    "########### Plot decision bounaries ###########\n",
    "#################################################################\n",
    "##### Start code here #####\n",
    "\n",
    "##### End code here #####\n",
    "#################################################################\n",
    "\n",
    "########### Plot training data points ###########\n",
    "idx = 0\n",
    "label_idx = 0\n",
    "for label in data_labels:\n",
    "    plt.scatter(train_x_pca_2d[idx : idx + train_size[label], 0], \\\n",
    "                train_x_pca_2d[idx : idx + train_size[label], 1], \\\n",
    "                c = colors[label_idx], \\\n",
    "                label = 'train - ' + label)\n",
    "    idx = idx + train_size[label]\n",
    "    label_idx = label_idx + 1\n",
    "\n",
    "########### Plot testing data points ###########\n",
    "idx = 0\n",
    "label_idx = 0\n",
    "for label in data_labels:\n",
    "    plt.scatter(test_x_pca_2d[idx : idx + test_size[label], 0], \\\n",
    "                test_x_pca_2d[idx : idx + test_size[label], 1], \\\n",
    "                c = colors[label_idx], \\\n",
    "                label = 'test - ' + label, edgecolor='k')\n",
    "    idx = idx + test_size[label]\n",
    "    label_idx = label_idx + 1\n",
    "    \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Deep Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use Deep Neural Network (DNN) to classify attacks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 - Imoprt Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Convert Y labels to one-hot matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not.\n",
    "\n",
    "This is called **one hot encoding** from a categorical variable.\n",
    "\n",
    "<img src=\"./one-hot-encoding.png\"  style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_one_hot = np_utils.to_categorical(train_y)\n",
    "vali_y_one_hot = np_utils.to_categorical(vali_y)\n",
    "test_y_one_hot = np_utils.to_categorical(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_idx = 10\n",
    "print('The old class is {}, and the new class is {}'.format(train_y[example_idx], train_y_one_hot[example_idx,:]))\n",
    "\n",
    "example_idx = 300\n",
    "print('The old class is {}, and the new class is {}'.format(train_y[example_idx], train_y_one_hot[example_idx,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - Background on DNN\n",
    "\n",
    "We now discuss a couple of things you may need to know in order to implement a DNN using Keras.\n",
    "- A DNN usually consists of 1) **Input Layer**, 2) *L* **Hidden Layers** (L=3 in this example shown below), and 3) **Output Layer**.\n",
    "- Each layer consists of a certain numer of **neurons**.\n",
    "    - In this example, the input layer has 4 neurons.\n",
    "    - Each hidden layer has 6 neurons.\n",
    "    - The output layer has 4 neurons.\n",
    "- In particular, this example is a **fully connected** DNN, that is, each neuron in the previous layer is connected to each neuron in the next layer.\n",
    "- Each neuro performs the following operation: it computes the weighted sums of its inputs, adds a bias, and decides whether it should be \"fired\"or not.\n",
    "    - The behavior of \"firing\" is captured by the **activation function** ([Wikipedia](https://en.wikipedia.org/wiki/Activation_function)). \n",
    "    - Common activation functions include linear, sigmoid, ReLU, and softmax. \n",
    "    - More references: https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0\n",
    "\n",
    "<img src=\"./neural_network.png\"  style=\"width: 600px;\"/>\n",
    "\n",
    "- In addition, during training, we will update the model every several samples (i.e., batch) and pass the entire training set into the model multiple times (i.e., epochs).\n",
    "    - **Batch size**: Total number of training examples present in a single batch.\n",
    "    - **Epochs**: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE.\n",
    "\n",
    "(Reference: https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 - Task 8: Implement a DNN using Keras\n",
    "\n",
    "In this task, you will implement a 2-layer DNN using keras, based on whatever online resources you can find. A list of useful tutorials are provided as below:\n",
    "- https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "- https://www.tensorflow.org/tutorials/keras/basic_classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.1: Create a DNN model.**\n",
    "\n",
    "Specifications:\n",
    "- The DNN model (created using `Sequential()`) consists of 3 fully connected layers (using `Dense()`), in additiion to the input layer:\n",
    "    - The Input Layer consists of `num_features` neurons.\n",
    "    - The 1st Hidden Layer consists of `64` neurons, using `relu` as the activation function.\n",
    "    - The 2nd Hidden Layer consists of `16` neurons, using `relu` as the activation function.\n",
    "    - The Output Layer consists of `num_classes` neurons, using `softmax` as the activation function.\n",
    "- Use `compile()` to compile the model.\n",
    "- Use `'categorical_crossentropy'` as the loss function, use `'adam'` as the optimizer, and use `['accuracy']` as the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_features = train_x.shape[1]\n",
    "num_classes = train_y_one_hot.shape[1]\n",
    "\n",
    "model = None\n",
    "#################################################################\n",
    "##### Start code here #####\n",
    "\n",
    "##### End code here\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.2: Train the DNN model.**\n",
    "\n",
    "Speficiations:\n",
    "- You will need to call `model.fit()` of the created DNN model with proper arguments.\n",
    "- Train on `train_x` and `train_y_one_hot`\n",
    "- Use `(vali_x, vali_y_one_hot)` as the `validation_data`\n",
    "- Use `epcohs=30` and `batch_size=10`.\n",
    "- Save the output of `model.fit()` as `history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = None\n",
    "\n",
    "#################################################################\n",
    "##### Start code here #####\n",
    "\n",
    "##### End code here #####\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.3: Plot both training and validation accuracies.**\n",
    "\n",
    "Hints: \n",
    "- To retrieve the training and validation accuracies for different epochs, use `history.history['acc']` and `history.history['val_acc']`\n",
    "- You will need to call `plt.plot()` for plotting.\n",
    "- You may need to call `plt.title()`, `plt.xlabel()`, `plt.ylabel()`, and `plt.legend()` to provide necessary information on the plot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_acc = history.history['acc']\n",
    "validation_acc = history.history['val_acc']\n",
    "\n",
    "#################################################################\n",
    "##### Start code here #####\n",
    "\n",
    "##### End code here #####\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 8.4: Evaluate the model on the testing set and print the testing accuracy.**\n",
    "\n",
    "- Hint: You will need to call `model.evaluate()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################\n",
    "##### Start code here #####\n",
    "\n",
    "##### End code here #####\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Task 9: Additional Questions\n",
    "\n",
    "1. For the kNN classifier, there are several design parameters such as `n_neighbors` and `weights` that we can choose. Please describe how to leverage the validation set to choose the best design parameters. \n",
    "1. For DNN, what is the difference between different activation functions, specifically, linear, sigmoid, ReLU, vs. softmax ?\n",
    "1. In ML, What is overfitting and underfitting?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
